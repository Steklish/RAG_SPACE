# RAG AI Приложение

Это проект представляет собой простое приложение на основе архитектуры RAG (Retrieval-Augmented Generation), использующее локальный LLM-сервер, ChromaDB для векторного хранения и Pydantic для структурирования данных.

## RUN

Run chat model (example)
```shell
llama-server.exe --model D:\Duty\RR\models\gemma-3-1B-it-QAT-Q4_0.gguf --n_gpu_layers 999 --port 8080  --ctx-size 12000
```

Run embedding model (example)
```shell
llama-server --host 0.0.0.0 --port 11435 --model ..\..\..\RagFlow\models\embeddinggemma-300m-qat-Q8_0.gguf --embedding -c 2048 
```


## Основные классы

### `app/schemas.py`

Этот файл содержит Pydantic-схемы, которые определяют структуру данных в приложении.

-   `Document`: Представляет документ с его содержимым и метаданными.
-   `Chunk`: Представляет собой фрагмент (чанк) документа с текстовым содержанием, векторным представлением (embedding) и метаданными.
-   `Query`: Определяет структуру запроса пользователя, включая текст, количество извлекаемых результатов (`top_k`) и необязательные фильтры.
-   `Answer`: Структура для ответа, сгенерированного моделью, включая текст ответа и исходные чанки, на основе которых он был создан.

### `app/chroma_client.py`

-   `ChromaClient`: Класс-обертка для взаимодействия с векторной базой данных ChromaDB. Он управляет хранением, поиском, обновлением и удалением векторов (чанков).

### `app/embedding_client.py`

-   `EmbeddingClient`: Клиент для получения векторных представлений (embeddings) текста от локального LLM-сервера (например, llama.cpp).

### `app/local_generator.py`

-   `LocalGenerator`: Основной класс для генерации структурированных данных (Pydantic-моделей) с помощью локального LLM-сервера. Он формирует промпты, отправляет их на сервер и парсит JSON-ответы в объекты Pydantic.

## Базовый принцип работы

1.  **Индексация**: Документы разбиваются на чанки. Для каждого чанка с помощью `EmbeddingClient` получается векторное представление. Затем `ChromaClient` сохраняет эти чанки и их векторы в ChromaDB.
2.  **Запрос**: Пользователь отправляет запрос (`Query`).
3.  **Поиск**: `EmbeddingClient` создает вектор для текста запроса. `ChromaClient` использует этот вектор для поиска наиболее релевантных чанков в ChromaDB.
4.  **Генерация**: Найденные чанки вместе с исходным запросом передаются в `LocalGenerator` в качестве контекста. Модель генерирует ответ (`Answer`) на основе этого контекста.