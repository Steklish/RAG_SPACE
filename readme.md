# RAG AI-приложение

Это проект простого RAG-приложения (Retrieval-Augmented Generation), которое использует локальный LLM-сервер, ChromaDB для векторного хранения, бэкенд на FastAPI и пользовательский интерфейс на React.

## Основные компоненты

Приложение разделено на бэкенд на Python, отвечающий за логику ИИ, и фронтенд на React для взаимодействия с пользователем.

### Бэкенд (`app/`)

-   **`main.py`**: Сервер FastAPI, предоставляющий эндпоинты для загрузки документов, управления чат-тредами и взаимодействия с AI-агентом.
-   **`agent.py`**: Содержит класс `Agent`, который управляет определением намерений пользователя, извлечением документов и логикой генерации ответов.
-   **`chroma_client.py`**: Клиент для ChromaDB, который управляет двумя коллекциями: одна для текстовых чанков (`rag_collection`), а другая для метаданных документов (`documents_metadata`). Он также обрабатывает загрузку файлов, включая извлечение текста и его разделение на чанки.
-   **`embedding_client.py`**: Клиент для генерации текстовых эмбеддингов с использованием локального сервера эмбеддинг-модели.
-   **`local_generator.py`**: Клиент для генерации текстовых ответов от локального сервера чат-модели.
-   **`thread_store.py`**: Управляет тредами бесед, сохраняя и извлекая их из локальной файловой системы.
-   **`schemas.py`**: Определяет модели Pydantic, используемые для валидации и структурирования данных во всем бэкенде.

### Фронтенд (`UI/`)

-   Приложение на React, созданное с помощью Vite.
-   Использует Material-UI для компонентов и стилизации.
-   Имеет вкладочный интерфейс для "Чата" и "Настроек".
-   Включает изменяемые по размеру панели для гибкой компоновки пользовательского интерфейса.

## Как это работает

1.  **Загрузка документов**: Пользователи могут загружать документы через пользовательский интерфейс. Бэкенд сохраняет файл, извлекает из него текст, разбивает его на чанки, генерирует эмбеддинги для каждого чанка и сохраняет их в ChromaDB. Метаданные документа хранятся в отдельной коллекции.
2.  **Взаимодействие в чате**: Когда пользователь отправляет сообщение, `Agent` сначала определяет намерение пользователя и необходимость извлечения информации.
3.  **Извлечение**: Если извлечение необходимо, `Agent` использует `ChromaClient` для поиска релевантных текстовых чанков из проиндексированных документов.
4.  **Генерация**: Извлеченные чанки и история беседы передаются в `LocalGenerator`, который использует локальный LLM для генерации ответа.
5.  **Управление беседой**: `ThreadStore` отслеживает историю беседы для каждой сессии чата.

## Запуск приложения

Для запуска этого приложения необходимо запустить локальные серверы моделей, бэкенд FastAPI и фронтенд React.

### 1. Запуск серверов моделей

По умолчанию, серверы для чат-модели и эмбеддинг-модели запускаются автоматически вместе с бэкенд-сервером FastAPI. Конфигурации для запуска находятся в папке `app/launch_configs`.

Если вы хотите запустить бэкенд без автоматического запуска серверов моделей, установите переменную окружения `START_SERVERS` в `false`:

```shell
START_SERVERS=false python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8080
```

Если вы запустили приложение без серверов, вы можете запустить их позже, отправив POST-запрос на эндпоинт `/api/servers/start`.

Вы также можете запустить серверы моделей вручную. Вам понадобятся два отдельных экземпляра `llama-server`: один для чат-модели и один для эмбеддинг-модели.

**Запуск чат-модели (пример):**
```shell
llama-server.exe --model models/gemma-3-1B-it-QAT-Q4_0.gguf --n_gpu_layers 999 --port 11434 --ctx-size 12000
```

**Запуск эмбеддинг-модели (пример):**
```shell
llama-server --port 11435 --model models/embeddinggemma-300m-qat-Q8_0.gguf --embedding -c 2048 -b 2048 -ub 1024
```
*Примечание: При необходимости измените пути к моделям и параметры.*

### 2. Запуск бэкенд-сервера

Перейдите в корневой каталог проекта и запустите приложение FastAPI с помощью `uvicorn`.

```shell
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8080 
```
Бэкенд будет доступен по адресу `http://127.0.0.1:8080`.

### 3. Запуск фронтенд-интерфейса

В отдельном терминале перейдите в каталог `front-react-v`, установите зависимости и запустите сервер разработки Vite.

```shell
cd front-react-v
npm install
npm run dev
```
Пользовательский интерфейс будет доступен по адресу `http://localhost:5173` (или другому порту, если 5173 занят).


---
# ЗАЯВКА

**Название проекта:**
___ AI

**Опишите какую проблему решает ваш стартап (50 знаков или меньше):**
- Конфиденциальность данных при работе с AI-ассистентами.

**Цель вашего проекта?**
- Создать локальное RAG AI-приложение, которое позволяет бизнесу и индивидуальным пользователям безопасно работать с документами и базами знаний без отправки данных в облако, обеспечивая полный контроль и конфиденциальность.

**Почему вы решили работать над этой идеей? Есть ли у вас экспертные знания в этой области?**
- Мы видим растущую потребность в безопасных AI-решениях. Облачные LLM-сервисы не подходят для работы с конфиденциальной информацией. У нас есть опыт в разработке ПО, работе с machine learning моделями и управлении данными, что позволяет нам создать эффективный и надежный продукт.

**Есть ли у вас конкуренты? Что знаете вы, чего не знают или не умеют ваши конкуренты?**
- Основными конкурентами являются облачные AI-сервисы (ChatGPT, YandexGPT). Однако они требуют отправки данных на внешние серверы. Наше ключевое преимущество — полная локализация. Проект использует open-source технологии `llama.cpp` и `chroma DB`, что обеспечивает работу на собственном оборудовании. Это гарантирует максимальную безопасность и отсутствие зависимости от сторонних провайдеров.

**Какую пользу ваш проект может принести гражданам Беларуси?**
- Проект предоставит белорусским компаниям, особенно в финансовой, юридической и промышленной сферах, инструмент для внедрения AI без рисков утечки коммерческой тайны. Это повысит их конкурентоспособность. Также это стимулирует развитие локальных AI-компетенций и создаст основу для новых технологических продуктов в стране.

**На каком этапе вы находитесь и что вы планируете делать в ближайшее время?**
- Разработан прототип, который позволяет индексировать и выполнять запросы к локальной базе документов. В ближайшие 3 месяца мы планируем улучшить пользовательский интерфейс, расширить поддержку форматов документов и внедрить агентные функции.

**Нужны ли инвестиции на данном этапе? Если да, то в каком объёме (ориентировочная сумма) и на что они будут направлены?**
- Да. Они будут направлены на покупку оборудования для запуска локальных ИИ-моделей. (XXX$)

**Каковы сроки реализации проекта? Каковы сроки разработки первого MVP?**
- MVP уже готов. Публичный релиз с базовым функционалом планируется через 4 месяца. Версия с расширенными возможностями для корпоративных клиентов будет готова через 9-12 месяцев.

**Как вы зарабатываете/ собираетесь зарабатывать? Вы потенциально можете заработать?**
- ХЗ