# RAG AI-приложение

Это проект простого RAG-приложения (Retrieval-Augmented Generation), которое использует локальный LLM-сервер, ChromaDB для векторного хранения, бэкенд на FastAPI и пользовательский интерфейс на React.

## Основные компоненты

Приложение разделено на бэкенд на Python, отвечающий за логику ИИ, и фронтенд на React для взаимодействия с пользователем.

### Бэкенд (`app/`)

-   **`main.py`**: Сервер FastAPI, предоставляющий эндпоинты для загрузки документов, управления чат-тредами и взаимодействия с AI-агентом.
-   **`agent.py`**: Содержит класс `Agent`, который управляет определением намерений пользователя, извлечением документов и логикой генерации ответов.
-   **`chroma_client.py`**: Клиент для ChromaDB, который управляет двумя коллекциями: одна для текстовых чанков (`rag_collection`), а другая для метаданных документов (`documents_metadata`). Он также обрабатывает загрузку файлов, включая извлечение текста и его разделение на чанки.
-   **`embedding_client.py`**: Клиент для генерации текстовых эмбеддингов с использованием локального сервера эмбеддинг-модели.
-   **`local_generator.py`**: Клиент для генерации текстовых ответов от локального сервера чат-модели.
-   **`thread_store.py`**: Управляет тредами бесед, сохраняя и извлекая их из локальной файловой системы.
-   **`schemas.py`**: Определяет модели Pydantic, используемые для валидации и структурирования данных во всем бэкенде.

### Фронтенд (`UI/`)

-   Приложение на React, созданное с помощью Vite.
-   Использует Material-UI для компонентов и стилизации.
-   Имеет вкладочный интерфейс для "Чата" и "Настроек".
-   Включает изменяемые по размеру панели для гибкой компоновки пользовательского интерфейса.

## Как это работает

1.  **Загрузка документов**: Пользователи могут загружать документы через пользовательский интерфейс. Бэкенд сохраняет файл, извлекает из него текст, разбивает его на чанки, генерирует эмбеддинги для каждого чанка и сохраняет их в ChromaDB. Метаданные документа хранятся в отдельной коллекции.
2.  **Взаимодействие в чате**: Когда пользователь отправляет сообщение, `Agent` сначала определяет намерение пользователя и необходимость извлечения информации.
3.  **Извлечение**: Если извлечение необходимо, `Agent` использует `ChromaClient` для поиска релевантных текстовых чанков из проиндексированных документов.
4.  **Генерация**: Извлеченные чанки и история беседы передаются в `LocalGenerator`, который использует локальный LLM для генерации ответа.
5.  **Управление беседой**: `ThreadStore` отслеживает историю беседы для каждой сессии чата.

## Запуск приложения

Для запуска этого приложения необходимо запустить локальные серверы моделей, бэкенд FastAPI и фронтенд React.

### 1. Запуск серверов моделей

Вам понадобятся два отдельных экземпляра `llama-server`: один для чат-модели и один для эмбеддинг-модели.

**Запуск чат-модели (пример):**
```shell
llama-server.exe --model D:\Duty\RR\models\gemma-3-1B-it-QAT-Q4_0.gguf --n_gpu_layers 999 --port 11434 --ctx-size 12000
```

**Запуск эмбеддинг-модели (пример):**
```shell
llama-server --port 11435 --model D:\Duty\RR\models\embeddinggemma-300m-qat-Q8_0.gguf --embedding -c 2048 -b 2048 -ub 1024
```
*Примечание: При необходимости измените пути к моделям и параметры.*

### 2. Запуск бэкенд-сервера

Перейдите в корневой каталог проекта и запустите приложение FastAPI с помощью `uvicorn`.

```shell
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8080 
```
Бэкенд будет доступен по адресу `http://127.0.0.1:8080`.

### 3. Запуск фронтенд-интерфейса

В отдельном терминале перейдите в каталог `UI`, установите зависимости и запустите сервер разработки Vite.

```shell
cd UI
npm install
npm run dev
```
Пользовательский интерфейс будет доступен по адресу `http://localhost:5173` (или другому порту, если 5173 занят).